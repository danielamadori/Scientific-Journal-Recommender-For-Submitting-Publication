{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tMdvYrtPfZqF"
   },
   "source": [
    "# **Scientific journal recommender for submitting a publication**"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "\n",
    "folder = \"dataset/\"\n",
    "\n",
    "\n",
    "#from google.colab import drive\n",
    "#drive.mount('/content/drive')\n",
    "#colab_folder = 'drive/MyDrive/Colab Notebooks/ScientificJournalRecommenderForSubmittingPublication/'\n",
    "#folder = colab_folder + folder\n",
    "\n",
    "df = pd.read_csv(folder + 'selected_cleaned.csv')\n",
    "\n",
    "target = 'journal'\n",
    "\n",
    "features = df.columns.tolist()\n",
    "features.remove(target)\n",
    "\n",
    "# Text Representation: all the words of the selected features in a single column\n",
    "\n",
    "df['X'] = df[features[0]]\n",
    "for i in range(1, len(features)):\n",
    "    df['X'] = df['X'] + df[features[i]]\n",
    "\n",
    "# Remove null values\n",
    "df = df.dropna(subset=['X'])\n",
    "\n",
    "# Encode target_name\n",
    "labels = df[target].unique()\n",
    "df['y'] = df[target].replace(labels, list(range(len(labels))))\n",
    "\n",
    "# Remove unnecessary columns\n",
    "df = df[['X', 'y']]\n",
    "print(df.info())\n",
    "df.head()\n",
    "df.to_csv(folder + 'selected_cleaned_combined_text.csv', index=False)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from utils import word_frequency_analysis\n",
    "\n",
    "df = pd.read_csv(folder + 'selected_cleaned_combined_text.csv')\n",
    "\n",
    "# Number words for each frequency\n",
    "number_words_frequency = word_frequency_analysis(df, 'X', 15)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_81JoPc4Oz0l"
   },
   "source": [
    "# Divide the dataset into training and test sets"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jLhVc8ZkOz0n",
    "outputId": "8e4f2199-fcce-4e47-c6a1-b3160446a127",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1708296079546,
     "user_tz": 0,
     "elapsed": 795,
     "user": {
      "displayName": "Daniel Amadori",
      "userId": "06341863849919775088"
     }
    }
   },
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import gc\n",
    "\n",
    "df_train, df_test = train_test_split(df, train_size=0.8, random_state=42)\n",
    "# Delete df to free memory\n",
    "df = None\n",
    "gc.collect()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZajXFACiOz0p"
   },
   "source": [
    "## Bag of Words\n",
    "\n",
    "The **Bag of Words** is a representation of a text that describes the occurrence of words within a document.\n",
    "The Bag of Words is created using the following methods:\n",
    "* **CountVectorizer**: It counts the number of times a word appears in a document.\n",
    "* **TfidfVectorizer**: It counts the number of times a word appears in a document, but it also takes into account the frequency of the word in the entire corpus.\n",
    "\n",
    "Parameters:\n",
    "* **max_features**: build a vocabulary that only consider the top max_features ordered by term frequency across the corpus."
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "\n",
    "frequency = 13 #7\n",
    "max_features = number_words_frequency[frequency]\n",
    "print(f\"Words with frequency >= {frequency}: Max features: {max_features}\")\n",
    "\n",
    "bowCount = CountVectorizer(max_features=max_features)\n",
    "\n",
    "X_train_Count = bowCount.fit_transform(df_train['X']).toarray()\n",
    "X_test_Count = bowCount.transform(df_test['X']).toarray()\n",
    "\n",
    "bowTfidf = TfidfVectorizer(max_features=max_features)\n",
    "\n",
    "X_train_Tfidf = bowTfidf.fit_transform(df_train['X']).toarray()\n",
    "X_test_Tfidf = bowTfidf.transform(df_test['X']).toarray()\n",
    "\n",
    "y_train = df_train['y']\n",
    "y_test = df_test['y']"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wjbf5z8aiEUt"
   },
   "source": [
    "## Clasification\n",
    "\n",
    "The classification is performed using the **Random Forest** algorithm.\n",
    "It is an ensemble learning method that operates by constructing a multitude of decision trees at training time and outputting the class that is the mode of the classes (classification) or mean prediction (regression) of the individual trees."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 692
    },
    "id": "_8Q_UZSGM4av",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1708296083129,
     "user_tz": 0,
     "elapsed": 31,
     "user": {
      "displayName": "Daniel Amadori",
      "userId": "06341863849919775088"
     }
    },
    "outputId": "b496bc68-16f2-4b70-87db-e05565626008"
   },
   "source": [
    "from utils import evaluation_report, plot_class_distribution\n",
    "\n",
    "plot_class_distribution(y_train, labels)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# BoW ConuntVectorizer\n",
    "cls = RandomForestClassifier()\n",
    "cls.fit(X_train_Count, y_train)\n",
    "y_pred = cls.predict(X_test_Count)\n",
    "\n",
    "evaluation_report(y_test, y_pred, labels)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "G-6FH7iQM4ay",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1708296097382,
     "user_tz": 0,
     "elapsed": 14281,
     "user": {
      "displayName": "Daniel Amadori",
      "userId": "06341863849919775088"
     }
    },
    "outputId": "e4cb5a66-93bc-4ab6-8c58-294662b1b6a6"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "# BoW TfidfVectorizer\n",
    "cls = RandomForestClassifier()\n",
    "cls.fit(X_train_Tfidf, y_train)\n",
    "y_pred = cls.predict(X_test_Tfidf)\n",
    "\n",
    "evaluation_report(y_test, y_pred, labels)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "URkwEQaGQJCO",
    "outputId": "90bb0a87-fc81-41c2-e2c3-b8fdc8a764ca",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1708296112096,
     "user_tz": 0,
     "elapsed": 14761,
     "user": {
      "displayName": "Daniel Amadori",
      "userId": "06341863849919775088"
     }
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "tzMHykTyM4a3",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1708296113338,
     "user_tz": 0,
     "elapsed": 1359,
     "user": {
      "displayName": "Daniel Amadori",
      "userId": "06341863849919775088"
     }
    },
    "outputId": "49b9d962-40c9-48d3-8dd5-1a249863b252"
   },
   "source": [
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "\n",
    "sampler = RandomUnderSampler(random_state=42)\n",
    "\n",
    "# BoW ConuntVectorizer\n",
    "X_train_under_sampled, y_train_under_sampled = sampler.fit_resample(X_train_Count, y_train)\n",
    "plot_class_distribution(y_train_under_sampled, labels)\n",
    "\n",
    "cls = RandomForestClassifier()\n",
    "cls.fit(X_train_under_sampled, y_train_under_sampled)\n",
    "y_pred = cls.predict(X_test_Count)\n",
    "\n",
    "evaluation_report(y_test, y_pred, labels)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "import gc\n",
    "\n",
    "sampler = RandomUnderSampler(random_state=42)\n",
    "\n",
    "# BoW TfidfVectorizer\n",
    "X_train_under_sampled, y_train_under_sampled = sampler.fit_resample(X_train_Tfidf, y_train)\n",
    "\n",
    "cls = RandomForestClassifier()\n",
    "cls.fit(X_train_under_sampled, y_train_under_sampled)\n",
    "y_pred = cls.predict(X_test_Tfidf)\n",
    "\n",
    "evaluation_report(y_test, y_pred, labels)\n",
    "\n",
    "X_train_under_sampled = None\n",
    "y_train_under_sampled = None\n",
    "\n",
    "gc.collect()"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "7Ccq6Hb2oTEl",
    "outputId": "3add8810-9d8d-4571-e20d-f78de8437c67",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1708296115013,
     "user_tz": 0,
     "elapsed": 1694,
     "user": {
      "displayName": "Daniel Amadori",
      "userId": "06341863849919775088"
     }
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "jj9_bUbx2fAx",
    "outputId": "717274ae-ca7d-43e4-f9fb-7b091971d07d",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1708296153687,
     "user_tz": 0,
     "elapsed": 38699,
     "user": {
      "displayName": "Daniel Amadori",
      "userId": "06341863849919775088"
     }
    }
   },
   "source": [
    "from imblearn.over_sampling import RandomOverSampler\n",
    "\n",
    "sampler = RandomOverSampler(random_state=42)\n",
    "\n",
    "# BoW ConuntVectorizer\n",
    "X_train_over_sampled, y_train_over_sampled = sampler.fit_resample(X_train_Count, y_train)\n",
    "plot_class_distribution(y_train_over_sampled, labels)\n",
    "\n",
    "cls = RandomForestClassifier()\n",
    "cls.fit(X_train_over_sampled, y_train_over_sampled)\n",
    "y_pred = cls.predict(X_test_Count)\n",
    "\n",
    "evaluation_report(y_test, y_pred, labels)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "TNXGpF8AOz01",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "status": "ok",
     "timestamp": 1708296193968,
     "user_tz": 0,
     "elapsed": 40391,
     "user": {
      "displayName": "Daniel Amadori",
      "userId": "06341863849919775088"
     }
    },
    "outputId": "fe7c9d99-794a-4313-c23f-dda59274076e"
   },
   "source": [
    "from imblearn.over_sampling import RandomOverSampler\n",
    "\n",
    "sampler = RandomOverSampler(random_state=42)\n",
    "\n",
    "# BoW TfidfVectorizer\n",
    "X_train_over_sampled, y_train_over_sampled = sampler.fit_resample(X_train_Tfidf, y_train)\n",
    "\n",
    "cls = RandomForestClassifier()\n",
    "cls.fit(X_train_over_sampled, y_train_over_sampled)\n",
    "y_pred = cls.predict(X_test_Tfidf)\n",
    "\n",
    "evaluation_report(y_test, y_pred, labels)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dm7YXM_SOz06",
    "outputId": "e85934c4-25b1-4aaf-f9b1-5d0e1b9bb7c5",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1708296194437,
     "user_tz": 0,
     "elapsed": 626,
     "user": {
      "displayName": "Daniel Amadori",
      "userId": "06341863849919775088"
     }
    }
   },
   "source": [
    "# Cleaning memory\n",
    "import gc\n",
    "\n",
    "bowCount = None\n",
    "bowTfidf = None\n",
    "X_train = None\n",
    "X_test = None\n",
    "y_test = None\n",
    "y_train = None\n",
    "y_pred = None\n",
    "X_train_over_sampled = None\n",
    "y_train_over_sampled = None\n",
    "X_train_Count = None\n",
    "X_test_Count = None\n",
    "X_train_Tfidf = None\n",
    "X_test_Tfidf = None\n",
    "\n",
    "gc.collect()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Xk2zCYd5cmK4"
   },
   "source": [
    "\n",
    "## Connectionist techniques\n",
    "\n",
    "In this case, after pre-processing, a neural network based on an LSTM unit is trained."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "bjKZ7Y-g9tdH",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "240cd529-b027-443e-b457-2e36a83a289c",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1708296197057,
     "user_tz": 0,
     "elapsed": 2653,
     "user": {
      "displayName": "Daniel Amadori",
      "userId": "06341863849919775088"
     }
    }
   },
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "device = tf.config.list_physical_devices('GPU')\n",
    "\n",
    "\n",
    "folder = 'dataset/'\n",
    "\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "folder = 'drive/MyDrive/Colab Notebooks/ScientificJournalRecommenderForSubmittingPublication/' + folder\n",
    "\n",
    "df = pd.read_csv(folder + 'selected_cleaned.csv')\n",
    "\n",
    "target = 'journal'\n",
    "\n",
    "features = df.columns.tolist()\n",
    "features.remove(target)\n",
    "\n",
    "df.info()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "from utils import sentences_length_analysis\n",
    "from utils import word_frequency_analysis\n",
    "\n",
    "sentences_length = {}\n",
    "number_words_frequency = {}\n",
    "\n",
    "feature = features[0]\n",
    "sentences_length_analysis(df, feature, [0, 300])\n",
    "sentences_length_analysis(df, feature, [150, 300])\n",
    "sentences_length_analysis(df, feature, [250, 300])"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "Te7tXilyM4a-",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1708296199561,
     "user_tz": 0,
     "elapsed": 2527,
     "user": {
      "displayName": "Daniel Amadori",
      "userId": "06341863849919775088"
     }
    },
    "outputId": "e462bb79-b525-418a-bf0e-1729b91287bc"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "sentences_length[feature] = 280\n",
    "\n",
    "number_words_frequency[feature] = word_frequency_analysis(df, feature, 20)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 830
    },
    "id": "ocn4fMweM4a_",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1708296199562,
     "user_tz": 0,
     "elapsed": 104,
     "user": {
      "displayName": "Daniel Amadori",
      "userId": "06341863849919775088"
     }
    },
    "outputId": "038d0251-3149-4f7d-b250-f2aef504ff44"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "frequency = 4\n",
    "number_words_frequency[feature] = number_words_frequency[feature][frequency]\n",
    "\n",
    "print(f\"Feature: {feature}\")\n",
    "print(f\"Words with frequency >= {frequency}: {number_words_frequency[feature]}\")\n",
    "print(f\"Sentences length: {sentences_length[feature]}\")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YOa8VPYKM4bA",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1708296199563,
     "user_tz": 0,
     "elapsed": 92,
     "user": {
      "displayName": "Daniel Amadori",
      "userId": "06341863849919775088"
     }
    },
    "outputId": "5e44df35-ace7-4a66-84bb-a54863e65037"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "feature = features[1]\n",
    "\n",
    "sentences_length_analysis(df, feature, [0, 30])\n",
    "sentences_length_analysis(df, feature, [10, 25])\n",
    "sentences_length_analysis(df, feature, [15, 25])"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "ZQTqfqMrM4bB",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1708296201904,
     "user_tz": 0,
     "elapsed": 2427,
     "user": {
      "displayName": "Daniel Amadori",
      "userId": "06341863849919775088"
     }
    },
    "outputId": "f11c4e1a-ca9c-4080-f69d-2ff4334e34e2"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "sentences_length[feature] = 18\n",
    "\n",
    "number_words_frequency[feature] = word_frequency_analysis(df, feature, 25)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 917
    },
    "id": "RC2_msG5M4bC",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1708296201906,
     "user_tz": 0,
     "elapsed": 509,
     "user": {
      "displayName": "Daniel Amadori",
      "userId": "06341863849919775088"
     }
    },
    "outputId": "e474f8ea-3fae-4b0e-e62b-1712513af633"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "frequency = 2\n",
    "number_words_frequency[feature] = number_words_frequency[feature][frequency]\n",
    "\n",
    "print(f\"Feature: {feature}\")\n",
    "print(f\"Words with frequency >= {frequency}: {number_words_frequency[feature]}\")\n",
    "print(f\"Sentences length: {sentences_length[feature]}\")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VEVNVEx0M4bD",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1708296201912,
     "user_tz": 0,
     "elapsed": 425,
     "user": {
      "displayName": "Daniel Amadori",
      "userId": "06341863849919775088"
     }
    },
    "outputId": "c8e85c05-c839-4355-b72b-76a775a6ae2b"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "feature = features[2]\n",
    "\n",
    "sentences_length_analysis(df, feature, [0, 30])\n",
    "sentences_length_analysis(df, feature, [8, 25])\n",
    "sentences_length_analysis(df, feature, [8, 17])"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "gJuThZc1Oz1B",
    "outputId": "864ed2a0-523d-45c8-f7b3-1b17b814e204",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1708296204571,
     "user_tz": 0,
     "elapsed": 3070,
     "user": {
      "displayName": "Daniel Amadori",
      "userId": "06341863849919775088"
     }
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "sentences_length[feature] = 14\n",
    "\n",
    "number_words_frequency[feature] = word_frequency_analysis(df, feature, 20)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 829
    },
    "id": "Zio1cWqKM4bF",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1708296204572,
     "user_tz": 0,
     "elapsed": 1198,
     "user": {
      "displayName": "Daniel Amadori",
      "userId": "06341863849919775088"
     }
    },
    "outputId": "5f5be13f-ef7a-4c26-a64e-a3940ce77d5b"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "frequency = 2\n",
    "number_words_frequency[feature] = number_words_frequency[feature][frequency]\n",
    "\n",
    "print(f\"Feature: {feature}\")\n",
    "print(f\"Words with frequency >= {frequency}: {number_words_frequency[feature]}\")\n",
    "print(f\"Sentences length: {sentences_length[feature]}\")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Nn8yEyniM4bG",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1708296204572,
     "user_tz": 0,
     "elapsed": 1076,
     "user": {
      "displayName": "Daniel Amadori",
      "userId": "06341863849919775088"
     }
    },
    "outputId": "59837bd2-8515-45e2-a78c-ddf7194a28bf"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "# Convert list of words to string representation\n",
    "def join_elements(list_like_string):\n",
    "    return ' '.join(eval(list_like_string))\n",
    "\n",
    "for feature in features:\n",
    "    df[feature] = df[feature].apply(join_elements)\n",
    "\n",
    "df.fillna(\"\").astype(str)\n",
    "df.to_csv(folder + 'selected_cleaned_sentences.csv', index=False)"
   ],
   "metadata": {
    "id": "ruwy_9dKM4bG",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1708296206986,
     "user_tz": 0,
     "elapsed": 3474,
     "user": {
      "displayName": "Daniel Amadori",
      "userId": "06341863849919775088"
     }
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "0L9ujfipM4bI",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1708296207003,
     "user_tz": 0,
     "elapsed": 95,
     "user": {
      "displayName": "Daniel Amadori",
      "userId": "06341863849919775088"
     }
    }
   },
   "source": [
    "from keras.utils import to_categorical\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "df = pd.read_csv(folder + 'selected_cleaned_sentences.csv')\n",
    "\n",
    "target = 'journal'\n",
    "\n",
    "features = df.columns.tolist()\n",
    "features.remove(target)\n",
    "\n",
    "# Replacing labels names with int\n",
    "labels = df[target].unique()\n",
    "num_classes = len(labels)\n",
    "df[target] = df[target].replace(labels, list(range(len(labels))))\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(df[features], df[target], test_size=0.2, random_state=42)\n",
    "\n",
    "# Convert int labels to one-hot encoded format\n",
    "y_train = to_categorical(y_train, num_classes=num_classes)\n",
    "y_test = to_categorical(y_test, num_classes=num_classes)\n",
    "\n",
    "X_train = X_train.fillna(\"\").astype(str)\n",
    "X_test = X_test.fillna(\"\").astype(str)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# Tokenize the text for each feature\n",
    "def tokenize_text_feature(X_train, X_test, length_analysis, number_words_frequency):\n",
    "    print(f\"Number words: {number_words_frequency}\\t-\\tSequence length: {length_analysis}\")\n",
    "\n",
    "    tokenizer = Tokenizer(num_words=number_words_frequency)\n",
    "    tokenizer.fit_on_texts(X_train)\n",
    "\n",
    "    X_train_seq = tokenizer.texts_to_sequences(X_train)\n",
    "    X_train_pad = pad_sequences(X_train_seq, maxlen=length_analysis)\n",
    "\n",
    "    X_test_seq = tokenizer.texts_to_sequences(X_test)\n",
    "    X_test_pad = pad_sequences(X_test_seq, maxlen=length_analysis)\n",
    "\n",
    "    return X_train_pad, X_test_pad\n",
    "\n",
    "def tokenize_text(X_train, X_test, features, length_analysis, number_words_frequency):\n",
    "    X_train_pad = dict()\n",
    "    X_test_pad = dict()\n",
    "\n",
    "    for feature in features:\n",
    "        print('Feature: ' + feature)\n",
    "        X_train_pad[feature], X_test_pad[feature] = tokenize_text_feature(X_train[feature], X_test[feature], length_analysis[feature], number_words_frequency[feature])\n",
    "\n",
    "    return X_train_pad, X_test_pad\n",
    "\n",
    "\n",
    "X_train_pad, X_test_pad = tokenize_text(X_train, X_test, features, sentences_length, number_words_frequency)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KmpyV16ZOz1C",
    "outputId": "d13ba2c1-b16d-4338-b91f-cf804108feb0",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1708296209536,
     "user_tz": 0,
     "elapsed": 2626,
     "user": {
      "displayName": "Daniel Amadori",
      "userId": "06341863849919775088"
     }
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from keras.models import Model\n",
    "from keras.layers import Input, Embedding, LSTM, Dense, concatenate, Dropout, Bidirectional, BatchNormalization\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from metrics import f1_score\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def create_model(features, length_analysis, number_words, embedding_dims, lstm_units, dense_sizes, dropouts, num_classes, use_normalization=False):\n",
    "    inputs = []\n",
    "    models = []\n",
    "\n",
    "    model_name = 'a'#f\"{'Norm_' if use_normalization else ''}Embed_{'_'.join([f'{feat}{dim}' for feat, dim in embedding_dims.items()])}_LSTM_{'_'.join([f'{feat}{unit}' for feat, unit in lstm_units.items()])}_Dense_{'_'.join(map(str, dense_sizes))}_Drop_{'_'.join(map(str, dropouts))}\"\n",
    "\n",
    "    print(\"Creating model: \", model_name)\n",
    "\n",
    "    for feature in features:\n",
    "        input_layer = Input(shape=(length_analysis[feature],), name=feature)\n",
    "        inputs.append(input_layer)\n",
    "\n",
    "        embedding_layer = Embedding(\n",
    "            input_dim=number_words[feature],\n",
    "            output_dim=embedding_dims[feature],\n",
    "            input_length=length_analysis[feature],\n",
    "            name=f'{feature}_Embedding'\n",
    "        )(input_layer)\n",
    "\n",
    "        lstm_layer = Bidirectional(LSTM(\n",
    "            units=lstm_units[feature],\n",
    "            dropout=0.2,\n",
    "            recurrent_dropout=0.2),\n",
    "            name=f'{feature}_Bidirectional_LSTM'\n",
    "        )(embedding_layer)\n",
    "        models.append(lstm_layer)\n",
    "\n",
    "    concatenated_features = concatenate(models, name='Concatenate_Features')\n",
    "    if use_normalization:\n",
    "        concatenated_features = BatchNormalization(name='Batch_Normalization')(concatenated_features)\n",
    "\n",
    "    for i, (dense_size, dropout) in enumerate(zip(dense_sizes, dropouts)):\n",
    "        if i == 0:\n",
    "            dense_layer = Dense(dense_size, activation='relu', name=f'Dense_{i+1}')(concatenated_features)\n",
    "        else:\n",
    "            dense_layer = Dense(dense_size, activation='relu', name=f'Dense_{i+1}')(dropout_layer)\n",
    "\n",
    "        dropout_layer = Dropout(dropout, name=f'Dropout_{i+1}')(dense_layer)\n",
    "\n",
    "    output_layer = Dense(units=num_classes, activation='softmax', name=f'Output')(dropout_layer)\n",
    "\n",
    "    model = Model(inputs=inputs, outputs=output_layer, name=model_name)\n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=[f1_score])\n",
    "    print(model.summary())\n",
    "\n",
    "    return model\n",
    "\n",
    "def train_model(model, X_train, y_train, X_test, y_test, batch_size=64, epochs=20, patience=10):\n",
    "    print(f\"Training model: {model.name}\")\n",
    "\n",
    "    history = model.fit(X_train, y_train,\n",
    "                        batch_size=batch_size,\n",
    "                        epochs=epochs,\n",
    "                        validation_data=(X_test, y_test),\n",
    "                        callbacks=[\n",
    "                            EarlyStopping(monitor='val_loss', patience=patience, restore_best_weights=True)\n",
    "                        ])\n",
    "\n",
    "    # Plot Loss\n",
    "    plt.figure(figsize=(10, 4))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(history.history['loss'], label='Train Loss')\n",
    "    plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "    plt.title(f'Model Loss: {model.name}')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend(['Train', 'Validation'], loc='upper right')\n",
    "    plt.grid(True)\n",
    "\n",
    "    # Plot F1 Score\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(history.history['f1_score'], label='Train F1 Score')\n",
    "    plt.plot(history.history['val_f1_score'], label='Validation F1 Score')\n",
    "    plt.title(f'Model F1 Score: {model.name}')\n",
    "    plt.ylabel('F1 Score')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend(['Train', 'Validation'], loc='upper left')\n",
    "    plt.grid(True)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # Return best validation F1 score\n",
    "    score = max(history.history['val_f1_score'])\n",
    "    print(score)\n",
    "\n",
    "    return score\n",
    "\n",
    "def evaluate_model(model, X_test, y_test, labels):\n",
    "    print(f\"Evaluating model: {model.name}\")\n",
    "    # Make predictions\n",
    "    y_pred = model.predict(X_test)\n",
    "    y_pred = np.argmax(y_pred, axis=1)\n",
    "\n",
    "    return evaluation_report(y_test, y_pred, labels)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "!pip install deap",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "models = {}\n",
    "evaluation_reports = {}\n",
    "\n",
    "import random\n",
    "from deap import base, creator, tools, algorithms\n",
    "\n",
    "# Define hyperparameter ranges for each feature\n",
    "embedding_dim_ranges = {\n",
    "    features[0]: [150, 200],\n",
    "    features[1]: [100, 150],\n",
    "    features[2]: [100, 150]\n",
    "}\n",
    "\n",
    "lstm_units_ranges = {\n",
    "    features[0]: [100, 200, 300],\n",
    "    features[1]: [10, 20, 30],\n",
    "    features[2]: [10, 20, 30]\n",
    "}\n",
    "\n",
    "dense_sizes_range = [[64, 128], [128, 256]]\n",
    "\n",
    "dropout_range = [[0.1, 0.3], [0.2, 0.4], [0.4, 0.6]]\n",
    "\n",
    "# Define the fitness function\n",
    "def evalModel(individual):\n",
    "    # Extract feature-specific embedding dimensions and LSTM units\n",
    "    num_features = len(features)\n",
    "    embedding_dims = {features[i]: individual[2*i] for i in range(num_features)}\n",
    "    lstm_units = {features[i]: individual[2*i + 1] for i in range(num_features)}\n",
    "\n",
    "    dense_size = individual[-2]\n",
    "    dropout_rate = individual[-1]\n",
    "\n",
    "    print(\"Dens size: \", dense_size)\n",
    "    print(\"Dens size: \", type(dense_size))\n",
    "\n",
    "    # Create the model using the extracted hyperparameters\n",
    "    model = create_model(features, sentences_length, number_words_frequency, embedding_dims, lstm_units, dense_size, dropout_rate, num_classes, use_normalization=False)\n",
    "\n",
    "    # Train the model and return the best validation F1 score\n",
    "    score = train_model(model, X_train, y_train, epochs=20, batch_size=128)\n",
    "\n",
    "    # Evaluate the model using your predefined evaluate_model function\n",
    "    f1_score = evaluate_model(model, X_test, y_test)  # This function should return the F1-score\n",
    "\n",
    "    # The genetic algorithm is maximizing the fitness, so return the F1-score as a tuple\n",
    "    return (f1_score,)\n",
    "\n",
    "\n",
    "# Set up the genetic algorithm\n",
    "creator.create(\"FitnessMax\", base.Fitness, weights=(1.0,))\n",
    "creator.create(\"Individual\", list, fitness=creator.FitnessMax)\n",
    "\n",
    "toolbox = base.Toolbox()\n",
    "\n",
    "# Attribute generators\n",
    "for i, feature in enumerate(features):\n",
    "    toolbox.register(f\"attr_embedding_{i}\", random.choice, embedding_dim_ranges[feature])\n",
    "    toolbox.register(f\"attr_lstm_{i}\", random.choice, lstm_units_ranges[feature])\n",
    "\n",
    "toolbox.register(f\"attr_dense\", random.choice, dense_sizes_range)\n",
    "toolbox.register(f\"attr_dropout\", random.choice, dropout_range)\n",
    "\n",
    "# Structure initializers\n",
    "toolbox.register(\"individual\", tools.initCycle, creator.Individual,\n",
    "                 (toolbox.attr_embedding_0, toolbox.attr_embedding_1, toolbox.attr_embedding_2,\n",
    "                  toolbox.attr_lstm_0, toolbox.attr_lstm_1, toolbox.attr_lstm_2,\n",
    "                  toolbox.attr_dense,\n",
    "                  toolbox.attr_dropout\n",
    "                  ), n=1)\n",
    "toolbox.register(\"population\", tools.initRepeat, list, toolbox.individual)\n",
    "\n",
    "toolbox.register(\"evaluate\", evalModel)\n",
    "toolbox.register(\"mate\", tools.cxTwoPoint)\n",
    "toolbox.register(\"mutate\", tools.mutGaussian, mu=0, sigma=1, indpb=0.1)\n",
    "toolbox.register(\"select\", tools.selTournament, tournsize=3)\n",
    "\n",
    "# Generate the population\n",
    "population = toolbox.population(n=50)\n",
    "\n",
    "# Evolution parameters\n",
    "ngen = 40\n",
    "result = algorithms.eaSimple(population, toolbox, cxpb=0.5, mutpb=0.2, ngen=ngen, verbose=True)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "_mGOfMANM4bM",
    "executionInfo": {
     "status": "error",
     "timestamp": 1708296938169,
     "user_tz": 0,
     "elapsed": 1250,
     "user": {
      "displayName": "Daniel Amadori",
      "userId": "06341863849919775088"
     }
    },
    "outputId": "8aa15937-ef7a-4a91-a37b-62a1c8390683"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "id": "5SLuEL57M4bO",
    "executionInfo": {
     "status": "aborted",
     "timestamp": 1708296737336,
     "user_tz": 0,
     "elapsed": 15,
     "user": {
      "displayName": "Daniel Amadori",
      "userId": "06341863849919775088"
     }
    }
   },
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
