{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tMdvYrtPfZqF"
   },
   "source": [
    "# **Scientific journal recommender for submitting a publication**"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "NAniabkkfix9",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "fac001d6-2994-46e2-dbaa-96f9be1f6338"
   },
   "source": [
    "import string\n",
    "\n",
    "folder = \"dataset/\"\n",
    "folder_raw = \"dataset_raw/\"\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-9Ud8vp4Ozz8"
   },
   "source": [
    "# Dataset\n",
    "\n",
    "For each class (journal) there is a file in BibTeX format containing the articles published in that journal. Each file was cleaned and formatted with the following online tool [BibTeX Tidy](https://flamingtempura.github.io/bibtex-tidy/index.html).\n",
    "\n",
    "Each article is represented by a record with the following fields:\n",
    "* **abstract**: Abstract of the article.\n",
    "* **author**: Author of the article.\n",
    "* **ENTRYTYPE**: Type of entry (article, book, inproceedings, etc.).\n",
    "* **doi**: Digital Object Identifier of the article.\n",
    "* **ID**: Unique identifier of the article.\n",
    "* **issn**: International Standard Serial Number of the journal in which the article was published.\n",
    "* **journal**: Journal in which the article was published.\n",
    "* **keywords**: Keywords of the article.\n",
    "* **note**: Additional information about the article.\n",
    "* **pages**: Pages of the article.\n",
    "* **title**: Title of the article.\n",
    "* **url**: URL of the article.\n",
    "* **volume**: Volume of the journal in which the article was published.\n",
    "* **year**: Year of publication of the article.\n",
    "\n",
    "The goal is to create a model that is able to predict the **journal** in which it will be published."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "4-CVi1n56Ygf"
   },
   "source": [
    "import os\n",
    "import bibtexparser\n",
    "import pandas as pd\n",
    "\n",
    "def read_bib_to_dataframe(file_path):\n",
    "    #with open(file_path, 'r', encoding='utf-8') as bibtex_file:\n",
    "    with open(file_path, 'r', encoding='latin-1') as bibtex_file:\n",
    "        return bibtexparser.load(bibtex_file)\n",
    "\n",
    "for filename in os.listdir(folder_raw):\n",
    "    if filename.endswith(\".bib\"):\n",
    "        filename_path = os.path.join(folder_raw, filename)\n",
    "        bib_data = read_bib_to_dataframe(filename_path)\n",
    "        if bib_data.entries:\n",
    "            df = pd.DataFrame(bib_data.entries)\n",
    "            df.to_csv(os.path.splitext(filename_path)[0] + '.csv', index=False)\n",
    "        else:\n",
    "            print(\"Error: \", filename, \" is empty\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "u-T9-8l-Oz0H",
    "outputId": "e76da136-7339-4cc5-9e11-3d285c941406"
   },
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "dfs = []\n",
    "for filename in os.listdir(folder_raw):\n",
    "    if filename.endswith(\".csv\"):\n",
    "        dfs.append(pd.read_csv(os.path.join(folder_raw, filename)))\n",
    "\n",
    "df = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "# Use the following id features to remove duplicates\n",
    "for feature in ['doi', 'ID']:\n",
    "    # Remove duplicates based on the subset of non-null, non-na, and non-empty values in the feature\n",
    "    tmp_df = df[df[feature].notnull() & df[feature].notna() & (df[feature] != '')]\n",
    "    duplicates_count = tmp_df.duplicated(subset=[feature]).sum()\n",
    "\n",
    "    if duplicates_count > 0:\n",
    "        print(f'Duplicates found using {feature}: {duplicates_count}\\n\\n')\n",
    "\n",
    "        df = df[~df[feature].duplicated(keep='first') | df[feature].isnull() | df[feature].isna() | (df[feature] == '')]\n",
    "\n",
    "print(df.info())\n",
    "df.head()\n",
    "df.to_csv(folder_raw + 'all.csv', index=False)\n",
    "\n",
    "for tmp_df in dfs:\n",
    "    tmp_df = None\n",
    "dfs = None"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VBAefxsfOz0P"
   },
   "source": [
    "# Feature Selection\n",
    "\n",
    "The following features are selected:\n",
    "* **abstract**: Abstract of the article.\n",
    "* **keywords**: Keywords of the article.\n",
    "* **title**: Title of the article.\n",
    "\n",
    "The target feature is:\n",
    "* **journal**: Journal in which the article was published."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "I7xjj5RY6V_p",
    "outputId": "175ba69b-39e9-4f28-c2d2-f0133034fb44"
   },
   "source": [
    "# Removing unnecessary columns\n",
    "import pandas as pd\n",
    "df = pd.read_csv(folder_raw + 'all.csv')\n",
    "\n",
    "feature_names = ['abstract', 'keywords', 'title']\n",
    "target_name = 'journal'\n",
    "\n",
    "# Remove all the row that countains null values in the target_name column\n",
    "df = df.dropna(subset=[target_name])\n",
    "\n",
    "df = df[feature_names + [target_name]]\n",
    "print(df.info())\n",
    "df.head()\n",
    "\n",
    "df.to_csv(folder_raw + 'selected.csv', index=False)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aG1BuPv2oREW",
    "outputId": "fa96ee05-b050-4ed8-c667-61a58256e9fd"
   },
   "source": [
    "# Cleaning data\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download('omw-1.4')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt_tab')\n",
    "\n",
    "language = 'english'\n",
    "# Convert to lowercase\n",
    "df[feature_names] = df[feature_names].applymap(lambda x: str(x).lower())\n",
    "# Remove stopwords\n",
    "nltk.download('stopwords')\n",
    "stopwords_list = stopwords.words(language)\n",
    "df[feature_names] = df[feature_names].apply(lambda x: x.apply(lambda words: ' '.join([w for w in words.split() if w not in stopwords_list])))\n",
    "# Remove punctuation\n",
    "nltk.download('punkt')\n",
    "df[feature_names] = df[feature_names].apply(lambda x: x.str.translate(str.maketrans('', '', string.punctuation)))\n",
    "# Stemming\n",
    "stemmer = nltk.stem.SnowballStemmer(language=language)\n",
    "df[feature_names] = df[feature_names].apply(lambda x: x.apply(lambda words: ' '.join([stemmer.stem(w) for w in words.split()])))\n",
    "# Tokenize\n",
    "df[feature_names] = df[feature_names].apply(lambda x: x.apply(nltk.word_tokenize))\n",
    "\n",
    "df.head()\n",
    "df.to_csv(folder + 'selected_cleaned.csv', index=False)"
   ],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
